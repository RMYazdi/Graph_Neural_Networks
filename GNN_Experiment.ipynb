{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RMYazdi/Graph_Neural_Networks/blob/main/GNN_Experiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Install depedancies"
      ],
      "metadata": {
        "id": "Wdgqvin3P-Ca"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCLbgz7b_UxQ"
      },
      "outputs": [],
      "source": [
        "# Install required packages.\n",
        "# !pip install -q torch-scatter -f https://data.pyg.org/whl/torch-1.10.0+cu113.html\n",
        "# !pip install -q torch-sparse -f https://data.pyg.org/whl/torch-1.10.0+cu113.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.transforms import NormalizeFeatures\n",
        "# Helper function for visualization.\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "import torch\n",
        "from torch.nn import Linear\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.nn import SAGEConv\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix,log_loss\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from IPython.display import Javascript  # Restrict height of output cell.\n",
        "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 10})'''))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def visualize(h, color):\n",
        "    z = TSNE(n_components=2).fit_transform(h.detach().cpu().numpy())\n",
        "\n",
        "    plt.figure(figsize=(10,10))\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "\n",
        "    plt.scatter(z[:, 0], z[:, 1], s=70, c=color, cmap=\"Set2\")\n",
        "    plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EsYSOjC4CEhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Cora Dataset\n",
        "To demonstrate, we make use of the `Cora` dataset, which is a **citation network** where nodes represent documents.\n",
        "Each node is described by a 1433-dimensional bag-of-words feature vector.\n",
        "Two documents are connected if there exists a citation link between them.\n",
        "The task is to infer the category of each document (7 in total). We can see that the `Cora` network holds 2,708 nodes and 10,556 edges, resulting in an average node degree of 3.9."
      ],
      "metadata": {
        "id": "mP2ZLZIE_VYB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = Planetoid(root='data/Planetoid', name='Cora', transform=NormalizeFeatures())\n",
        "\n",
        "print()\n",
        "print(f'Dataset: {dataset}:')\n",
        "print('======================')\n",
        "print(f'Number of graphs: {len(dataset)}')\n",
        "print(f'Number of features: {dataset.num_features}')\n",
        "print(f'Number of classes: {dataset.num_classes}')\n",
        "\n",
        "data = dataset[0]  # Get the first graph object.\n",
        "\n",
        "print()\n",
        "print(data)\n",
        "print('===========================================================================================================')\n",
        "\n",
        "# Gather some statistics about the graph.\n",
        "print(f'Number of nodes: {data.num_nodes}')\n",
        "print(f'Number of edges: {data.num_edges}')\n",
        "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
        "print(f'Number of training nodes: {data.train_mask.sum()}')\n",
        "print(f'Training node label rate: {int(data.train_mask.sum()) / data.num_nodes:.2f}')\n",
        "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
        "print(f'Has self-loops: {data.has_self_loops()}')\n",
        "print(f'Is undirected: {data.is_undirected()}')"
      ],
      "metadata": {
        "id": "84BGSPx5UUuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Architecture"
      ],
      "metadata": {
        "id": "2kkME2rHB6pM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MLP Architecture"
      ],
      "metadata": {
        "id": "Oygep8yBB9Ar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class MLP(torch.nn.Module):\n",
        "    def __init__(self,num_input, hidden_channels):\n",
        "        super().__init__()\n",
        "        torch.manual_seed(12345)\n",
        "        self.lin1 = Linear(num_input, hidden_channels)\n",
        "        self.lin2 = Linear(hidden_channels, dataset.num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.lin1(x)\n",
        "        x = x.relu()\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.lin2(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "gNx4Ljy2_dl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GNN Architecture"
      ],
      "metadata": {
        "id": "VPvCHjQaCKPJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self,num_input, hidden_channels):\n",
        "        super().__init__()\n",
        "        torch.manual_seed(1234567)\n",
        "        self.conv1 = GCNConv(num_input, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, dataset.num_classes)\n",
        "\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "O6nhMfsvCS4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Canonical Feature + GNN**"
      ],
      "metadata": {
        "id": "iXu8w0TcEkFd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_data=data.x\n",
        "input_data=input_data.float()\n",
        "\n",
        "\n",
        "model = GCN(input_data.shape[1],hidden_channels=16)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "def train():\n",
        "      model.train()\n",
        "      optimizer.zero_grad()  # Clear gradients.\n",
        "      out = model(input_data, data.edge_index)  # Perform a single forward pass.\n",
        "      loss = criterion(out[data.train_mask], data.y[data.train_mask])  # Compute the loss solely based on the training nodes.\n",
        "      loss.backward()  # Derive gradients.\n",
        "      optimizer.step()  # Update parameters based on gradients.\n",
        "      return loss\n",
        "\n",
        "def train_test_acc():\n",
        "      model.eval()\n",
        "      out = model(input_data, data.edge_index)\n",
        "      pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
        "\n",
        "      train_correct = pred[data.train_mask] == data.y[data.train_mask]  # Check against ground-truth labels.\n",
        "      train_acc = int(train_correct.sum()) / int(data.train_mask.sum())  # Derive ratio of correct predictions.\n",
        "\n",
        "      test_correct = pred[data.test_mask] == data.y[data.test_mask]  # Check against ground-truth labels.\n",
        "      test_acc = int(test_correct.sum()) / int(data.test_mask.sum())  # Derive ratio of correct predictions.\n",
        "      return train_acc,test_acc\n",
        "\n",
        "\n",
        "\n",
        "train_acc_list=[]\n",
        "test_acc_list=[]\n",
        "for epoch in range(1, 120):\n",
        "    loss = train()\n",
        "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')\n",
        "    train_acc,test_acc = train_test_acc()\n",
        "    train_acc_list.append(train_acc)\n",
        "    test_acc_list.append(test_acc)\n",
        "\n",
        "\n",
        "print(f'Test Accuracy: {test_acc:.4f}')"
      ],
      "metadata": {
        "id": "jnyMedPUDymQ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = model(data.x, data.edge_index)\n",
        "visualize(out, color=data.y)"
      ],
      "metadata": {
        "id": "1hutGMpdHqt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Canonical Featue + MLP**"
      ],
      "metadata": {
        "id": "Kkwxb7oqFUMF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_Train=data.x[data.train_mask]\n",
        "Y_Train=data.y[data.train_mask]\n",
        "X_Test=data.x[data.test_mask]\n",
        "Y_Test=data.y[data.test_mask]"
      ],
      "metadata": {
        "id": "K9dWWvmwEjap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_data=data.x\n",
        "input_data=input_data.float()\n",
        "\n",
        "model = MLP(input_data.shape[1],hidden_channels=16)\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()  # Define loss criterion.\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)  # Define optimizer.\n",
        "\n",
        "def train():\n",
        "      model.train()\n",
        "      optimizer.zero_grad()  # Clear gradients.\n",
        "      out = model(data.x)  # Perform a single forward pass.\n",
        "      loss = criterion(out[data.train_mask], data.y[data.train_mask])  # Compute the loss solely based on the training nodes.\n",
        "      loss.backward()  # Derive gradients.\n",
        "      optimizer.step()  # Update parameters based on gradients.\n",
        "      return loss\n",
        "\n",
        "def test():\n",
        "      model.eval()\n",
        "      out = model(data.x)\n",
        "      pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
        "      test_correct = pred[data.test_mask] == data.y[data.test_mask]  # Check against ground-truth labels.\n",
        "      test_acc = int(test_correct.sum()) / int(data.test_mask.sum())  # Derive ratio of correct predictions.\n",
        "      return test_acc\n",
        "\n",
        "for epoch in range(1, 120):\n",
        "    loss = train()\n",
        "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')\n",
        "\n",
        "test_acc = test()\n",
        "print(f'MLP Accuracy: {test_acc:.4f}')"
      ],
      "metadata": {
        "id": "ZNes4_FyFTmD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Canonical Featue + RandomForest**"
      ],
      "metadata": {
        "id": "79lzukvVEyvN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf_clf = RandomForestClassifier(criterion='entropy')\n",
        "rf_clf.fit(X_Train,Y_Train)\n",
        "y_predict = rf_clf.predict(X_Test)\n",
        "\n",
        "print(\"RandromForest Accuracy:\", accuracy_score(Y_Test,y_predict))\n"
      ],
      "metadata": {
        "id": "Zw0WIsabEpVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**PCA + GNN**"
      ],
      "metadata": {
        "id": "vprXFyg5Dwpf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = data.x\n",
        "pca = PCA(n_components=15)\n",
        "data_PCA=pca.fit_transform(X)"
      ],
      "metadata": {
        "id": "NAv0ON11DCoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "input_data=torch.tensor(data_PCA)\n",
        "input_data=input_data.float()\n",
        "\n",
        "model = GCN(input_data.shape[1],hidden_channels=16)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "def train():\n",
        "      model.train()\n",
        "      optimizer.zero_grad()  # Clear gradients.\n",
        "      out = model(input_data, data.edge_index)  # Perform a single forward pass.\n",
        "      loss = criterion(out[data.train_mask], data.y[data.train_mask])  # Compute the loss solely based on the training nodes.\n",
        "      loss.backward()  # Derive gradients.\n",
        "      optimizer.step()  # Update parameters based on gradients.\n",
        "      return loss\n",
        "\n",
        "def test():\n",
        "      model.eval()\n",
        "      out = model(input_data, data.edge_index)\n",
        "      pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
        "      test_correct = pred[data.test_mask] == data.y[data.test_mask]  # Check against ground-truth labels.\n",
        "      test_acc = int(test_correct.sum()) / int(data.test_mask.sum())  # Derive ratio of correct predictions.\n",
        "      return test_acc\n",
        "\n",
        "\n",
        "for epoch in range(1, 300):\n",
        "    loss = train()\n",
        "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')\n",
        "\n",
        "test_acc = test()\n",
        "print(f'Test Accuracy: {test_acc:.4f}')"
      ],
      "metadata": {
        "id": "drmfZqpADCrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**#PCA + MLP**"
      ],
      "metadata": {
        "id": "wiiMrnOmD8FP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_Train=data_PCA[data.train_mask]\n",
        "Y_Train=data.y[data.train_mask]\n",
        "X_Test=data_PCA[data.test_mask]\n",
        "Y_Test=data.y[data.test_mask]"
      ],
      "metadata": {
        "id": "8Q17jXasDCtk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_data=torch.tensor(data_PCA)\n",
        "input_data=input_data.float()\n",
        "\n",
        "model = MLP(input_data.shape[1],hidden_channels=16)\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()  # Define loss criterion.\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)  # Define optimizer.\n",
        "\n",
        "def train():\n",
        "      model.train()\n",
        "      optimizer.zero_grad()  # Clear gradients.\n",
        "      out = model(input_data)  # Perform a single forward pass.\n",
        "      loss = criterion(out[data.train_mask], data.y[data.train_mask])  # Compute the loss solely based on the training nodes.\n",
        "      loss.backward()  # Derive gradients.\n",
        "      optimizer.step()  # Update parameters based on gradients.\n",
        "      return loss\n",
        "\n",
        "def test():\n",
        "      model.eval()\n",
        "      out = model(input_data)\n",
        "      pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
        "      test_correct = pred[data.test_mask] == data.y[data.test_mask]  # Check against ground-truth labels.\n",
        "      test_acc = int(test_correct.sum()) / int(data.test_mask.sum())  # Derive ratio of correct predictions.\n",
        "      return test_acc\n",
        "\n",
        "for epoch in range(1, 300):\n",
        "    loss = train()\n",
        "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')\n",
        "\n",
        "test_acc = test()\n",
        "print(f'MLP Accuracy: {test_acc:.4f}')"
      ],
      "metadata": {
        "id": "sY3uh-J_DyfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PCA + RandomForest**"
      ],
      "metadata": {
        "id": "MbAZmCf4EcFw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf_clf = RandomForestClassifier(criterion='entropy')\n",
        "rf_clf.fit(X_Train,Y_Train)\n",
        "y_predict = rf_clf.predict(X_Test)\n",
        "print(\"RandromForest Accuracy:\", accuracy_score(Y_Test,y_predict))\n"
      ],
      "metadata": {
        "id": "_9BVYWlnDyi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "#**One-Hot + GNN**\n"
      ],
      "metadata": {
        "id": "SkV0XxGIC76M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_ind=torch.tensor(np.asarray(range(data.x.shape[0])))\n",
        "data_one_hot = torch.nn.functional.one_hot(data_ind, num_classes=data_ind.shape[0])"
      ],
      "metadata": {
        "id": "skMY2EHyC3Mm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_data=data_one_hot\n",
        "input_data=input_data.float()\n",
        "\n",
        "model = GCN(input_data.shape[1],hidden_channels=16)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "def train():\n",
        "      model.train()\n",
        "      optimizer.zero_grad()  # Clear gradients.\n",
        "      out = model(input_data, data.edge_index)  # Perform a single forward pass.\n",
        "      loss = criterion(out[data.train_mask], data.y[data.train_mask])  # Compute the loss solely based on the training nodes.\n",
        "      loss.backward()  # Derive gradients.\n",
        "      optimizer.step()  # Update parameters based on gradients.\n",
        "      return loss\n",
        "\n",
        "def test():\n",
        "      model.eval()\n",
        "      out = model(input_data, data.edge_index)\n",
        "      pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
        "      test_correct = pred[data.test_mask] == data.y[data.test_mask]  # Check against ground-truth labels.\n",
        "      test_acc = int(test_correct.sum()) / int(data.test_mask.sum())  # Derive ratio of correct predictions.\n",
        "      return test_acc\n",
        "\n",
        "\n",
        "for epoch in range(1, 300):\n",
        "    loss = train()\n",
        "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')\n",
        "\n",
        "test_acc = test()\n",
        "print(f'Test Accuracy: {test_acc:.4f}')"
      ],
      "metadata": {
        "id": "-IAeCjfsC7Pl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**One-Hot + MLP**"
      ],
      "metadata": {
        "id": "9RGdRl9xDDPg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_Train=data_one_hot[data.train_mask]\n",
        "Y_Train=data.y[data.train_mask]\n",
        "X_Test=data_one_hot[data.test_mask]\n",
        "Y_Test=data.y[data.test_mask]"
      ],
      "metadata": {
        "id": "GZlOS1n2DAKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "input_data=data_one_hot\n",
        "input_data=input_data.float()\n",
        "\n",
        "model = MLP(input_data.shape[1],hidden_channels=16)\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()  # Define loss criterion.\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)  # Define optimizer.\n",
        "\n",
        "def train():\n",
        "      model.train()\n",
        "      optimizer.zero_grad()  # Clear gradients.\n",
        "      out = model(input_data)  # Perform a single forward pass.\n",
        "      loss = criterion(out[data.train_mask], data.y[data.train_mask])  # Compute the loss solely based on the training nodes.\n",
        "      loss.backward()  # Derive gradients.\n",
        "      optimizer.step()  # Update parameters based on gradients.\n",
        "      return loss\n",
        "\n",
        "def test():\n",
        "      model.eval()\n",
        "      out = model(input_data)\n",
        "      pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
        "      test_correct = pred[data.test_mask] == data.y[data.test_mask]  # Check against ground-truth labels.\n",
        "      test_acc = int(test_correct.sum()) / int(data.test_mask.sum())  # Derive ratio of correct predictions.\n",
        "      return test_acc\n",
        "\n",
        "for epoch in range(1, 300):\n",
        "    loss = train()\n",
        "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')\n",
        "\n",
        "test_acc = test()\n",
        "print(f'MLP Accuracy: {test_acc:.4f}')"
      ],
      "metadata": {
        "id": "Gy3q1HUlDCi_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**One-Hot + Random Forest**"
      ],
      "metadata": {
        "id": "oOl1opJ8DJRB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf_clf = RandomForestClassifier(criterion='entropy')\n",
        "rf_clf.fit(X_Train,Y_Train)\n",
        "y_predict = rf_clf.predict(X_Test)\n",
        "\n",
        "\n",
        "print(\"RandromForest Accuracy:\", accuracy_score(Y_Test,y_predict))\n"
      ],
      "metadata": {
        "id": "MEPFJOiGDClt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Index + GNN**"
      ],
      "metadata": {
        "id": "p-MhT-rICm4z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_index=torch.tensor(np.asarray(range(data.x.shape[0])))\n",
        "data_index=torch.reshape(data_index, (data_index.shape[0], 1))"
      ],
      "metadata": {
        "id": "38LhR4dtCVBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_index.shape"
      ],
      "metadata": {
        "id": "ZyyYg1ZhlEK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_data=data_index\n",
        "input_data=input_data.float()\n",
        "\n",
        "model = GCN(input_data.shape[1],hidden_channels=16)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "def train():\n",
        "      model.train()\n",
        "      optimizer.zero_grad()  # Clear gradients.\n",
        "      out = model(input_data, data.edge_index)  # Perform a single forward pass.\n",
        "      loss = criterion(out[data.train_mask], data.y[data.train_mask])  # Compute the loss solely based on the training nodes.\n",
        "      loss.backward()  # Derive gradients.\n",
        "      optimizer.step()  # Update parameters based on gradients.\n",
        "      return loss\n",
        "\n",
        "def train_test_acc():\n",
        "      model.eval()\n",
        "      out = model(input_data, data.edge_index)\n",
        "      pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
        "\n",
        "      train_correct = pred[data.train_mask] == data.y[data.train_mask]  # Check against ground-truth labels.\n",
        "      train_acc = int(train_correct.sum()) / int(data.train_mask.sum())  # Derive ratio of correct predictions.\n",
        "\n",
        "      test_correct = pred[data.test_mask] == data.y[data.test_mask]  # Check against ground-truth labels.\n",
        "      test_acc = int(test_correct.sum()) / int(data.test_mask.sum())  # Derive ratio of correct predictions.\n",
        "      return train_acc,test_acc\n",
        "\n",
        "\n",
        "\n",
        "train_acc_list=[]\n",
        "test_acc_list=[]\n",
        "for epoch in range(1, 120):\n",
        "    loss = train()\n",
        "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')\n",
        "    train_acc,test_acc = train_test_acc()\n",
        "    train_acc_list.append(train_acc)\n",
        "    test_acc_list.append(test_acc)\n",
        "\n",
        "\n",
        "print(f'Test Accuracy: {test_acc:.4f}')"
      ],
      "metadata": {
        "id": "vTefEX4_CrOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Index + MLP**"
      ],
      "metadata": {
        "id": "LSAu-R-dCvkl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_Train=data_index[data.train_mask]\n",
        "Y_Train=data.y[data.train_mask]\n",
        "X_Test=data_index[data.test_mask]\n",
        "Y_Test=data.y[data.test_mask]"
      ],
      "metadata": {
        "id": "aQocBix_Cs1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_data=data_index\n",
        "input_data=input_data.float()\n",
        "\n",
        "model = MLP(input_data.shape[1],hidden_channels=16)\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()  # Define loss criterion.\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)  # Define optimizer.\n",
        "\n",
        "def train():\n",
        "      model.train()\n",
        "      optimizer.zero_grad()  # Clear gradients.\n",
        "      out = model(input_data)  # Perform a single forward pass.\n",
        "      loss = criterion(out[data.train_mask], data.y[data.train_mask])  # Compute the loss solely based on the training nodes.\n",
        "      loss.backward()  # Derive gradients.\n",
        "      optimizer.step()  # Update parameters based on gradients.\n",
        "      return loss\n",
        "\n",
        "def test():\n",
        "      model.eval()\n",
        "      out = model(input_data)\n",
        "      pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
        "      test_correct = pred[data.test_mask] == data.y[data.test_mask]  # Check against ground-truth labels.\n",
        "      test_acc = int(test_correct.sum()) / int(data.test_mask.sum())  # Derive ratio of correct predictions.\n",
        "      return test_acc\n",
        "\n",
        "for epoch in range(1, 120):\n",
        "    loss = train()\n",
        "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')\n",
        "\n",
        "test_acc = test()\n",
        "print(f'MLP Accuracy: {test_acc:.4f}')"
      ],
      "metadata": {
        "id": "Lt5DsoXBCyRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Index + RandomForest**"
      ],
      "metadata": {
        "id": "j38vQhKDC1UE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf_clf = RandomForestClassifier(criterion='entropy')\n",
        "rf_clf.fit(X_Train,Y_Train)\n",
        "\n",
        "y_train_prdict=rf_clf.predict(X_Train)\n",
        "y_predict = rf_clf.predict(X_Test)\n",
        "\n",
        "print(\"RandromForest Train Accuracy:\", accuracy_score(Y_Train,y_train_prdict))\n",
        "print(\"RandromForest Test Accuracy:\", accuracy_score(Y_Test,y_predict))\n"
      ],
      "metadata": {
        "id": "XK4BAGdcCzUE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}